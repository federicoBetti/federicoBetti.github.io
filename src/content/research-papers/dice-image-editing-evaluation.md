---
title: "What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models"
authors: "L Baraldi, D Bucciarelli, F Betti, M Cornia, L Baraldi, N Sebe, R Cucchiara"
conference: "ICCV 2025"
pubDate: "May 26 2025"
description: "This paper introduces DICE (DIfference Coherence Estimator), a model designed to detect localized differences between original and edited images and assess their relevance to modification requests using Multimodal Large Language Models."
link: "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Ms5ctkUAAAAJ&citation_for_view=Ms5ctkUAAAAJ:LkGwnXOMwfcC"
tags: ["Computer Vision", "Image Editing", "Multimodal LLM", "Evaluation Metrics", "Human Judgment", "DICE", "Instruction-Guided", "Generative AI", "Self-Supervision", "Inpainting"]
pdfUrl: "https://arxiv.org/abs/2505.20405"
---

## Abstract

Instruction-based image editing models offer increased personalization opportunities in generative tasks. However, properly evaluating their results is challenging, and most of the existing metrics lag in terms of alignment with human judgment and explainability. To tackle these issues, we introduce DICE (DIfference Coherence Estimator), a model designed to detect localized differences between the original and the edited image and to assess their relevance to the given modification request. DICE consists of two key components: a difference detector and a coherence estimator, both built on an autoregressive Multimodal Large Language Model (MLLM) and trained using a strategy that leverages self-supervision, distillation from inpainting networks, and full supervision. Through extensive experiments, we evaluate each stage of our pipeline, comparing different MLLMs within the proposed framework. We demonstrate that DICE effectively identifies coherent edits, effectively evaluating images generated by different editing models with a strong correlation with human judgment. We publicly release our source code, models, and data.

## Key Contributions

1. Introduction of DICE (DIfference Coherence Estimator) for evaluating instruction-guided image edits
2. Novel approach combining difference detection with coherence estimation using MLLMs
3. Training strategy leveraging self-supervision, distillation, and full supervision
4. Strong correlation with human judgment in evaluating image editing results
5. Comprehensive evaluation framework for instruction-based image editing models
6. Public release of source code, models, and data for reproducibility 